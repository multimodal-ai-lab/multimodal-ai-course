{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsEC0XL7totx"
   },
   "source": [
    "# Exercise 4: (Multimodal) Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9beCUNRB9tBA"
   },
   "source": [
    "Install all requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T11:07:32.752166Z",
     "start_time": "2024-06-25T11:07:30.138669Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "gMTHGPN8tfkE",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c699c512-95a2-4620-93c7-d5a94a89734f"
   },
   "outputs": [],
   "source": [
    "! pip install accelerate\n",
    "! pip install transformers\n",
    "! pip install bitsandbytes\n",
    "! pip install duckduckgo_search\n",
    "! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepared a convenient wrapper class called `Model` which you can use to easily load an LLM from ðŸ¤— Hugging Face. We will use the 8B version of **Llama 3** because it is among the state-of-the-art in the class of open source LLMs with 8 billion parameters.\n",
    "\n",
    "Remember to set the `HUGGING_FACE_USER_ACCESS_TOKEN` as explained in the task instructions PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:54:28.398966Z",
     "start_time": "2024-06-25T14:54:25.836553Z"
    },
    "id": "lwLPlCBw_Nkr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, BitsAndBytesConfig\n",
    "\n",
    "HUGGING_FACE_USER_ACCESS_TOKEN = \"\"\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"\"\"Convenient wrapper, embodying the LLM.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str, task: str = 'text-generation',\n",
    "                 temperature: float = 0.2, top_k: int = 50,\n",
    "                 top_p: float = 0.9, revision=None):\n",
    "        self.name = name\n",
    "        self.temperature = temperature\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.max_prompt_len = 512\n",
    "        self.max_output_len = 512\n",
    "        self.task = task\n",
    "        self.revision = revision\n",
    "\n",
    "        self.pipeline = self.load(name)\n",
    "\n",
    "    def load(self, model_name: str):\n",
    "        \"\"\"Takes the Hugging Face model identifier and loads the corresponding model.\"\"\"\n",
    "\n",
    "        # Load the model in 4-bit precision to save computational resources\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "\n",
    "        pl = pipeline(\n",
    "            self.task,\n",
    "            max_new_tokens=self.max_output_len,\n",
    "            temperature=self.temperature,\n",
    "            top_k=self.top_k,\n",
    "            top_p=self.top_p,\n",
    "            model=model_name,\n",
    "            device_map=\"auto\",\n",
    "            token=HUGGING_FACE_USER_ACCESS_TOKEN,\n",
    "            truncation=True,\n",
    "            model_kwargs={\"quantization_config\": quantization_config},\n",
    "            revision=self.revision,\n",
    "        )\n",
    "\n",
    "        # Define the padding token\n",
    "        pl.tokenizer.pad_token_id = pl.tokenizer.eos_token_id\n",
    "\n",
    "        return pl\n",
    "\n",
    "    def generate(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Takes the prompt string (and optionally hyperparameters like temperature,\n",
    "        top_k, and top_p) and returns the string sequence continued by the LLM.\"\"\"\n",
    "\n",
    "        # Turn prompt into adequately formatted message\n",
    "        message = [{\"role\": \"user\", \"content\": prompt.strip()}]\n",
    "        message_formatted = self.pipeline.tokenizer.apply_chat_template(\n",
    "            message,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Insert prompt message into the LLM pipeline to generate an output\n",
    "        output = self.pipeline(message_formatted,\n",
    "            do_sample=True,\n",
    "            eos_token_id=self.pipeline.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.pipeline.tokenizer.pad_token_id,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Extract the response from the output message\n",
    "        return output[0]['generated_text'][len(message_formatted):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:58:39.768637Z",
     "start_time": "2024-06-24T15:58:29.134493Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "id": "i0bQhraKAqVV",
    "outputId": "da0bbc69-02df-42b0-e93e-d900a9b32118"
   },
   "outputs": [],
   "source": [
    "model = Model(\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:58:43.614232Z",
     "start_time": "2024-06-24T15:58:41.992523Z"
    },
    "id": "QcttXeJFGpPR"
   },
   "outputs": [],
   "source": [
    "print(model.generate(\"Write me a 4-line poem about Star Wars.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyBEb8_9tuSf"
   },
   "source": [
    "## Task 4.1: LLM Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJqSAgW9JQd7"
   },
   "source": [
    "### Task 4.1a) Basic Prompt Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yVlKTuq-pvi"
   },
   "source": [
    "Each of the following prompt best practices is accompanied by a scenario and a bad prompt. Your task is to come up with a better prompt according to the respective best practice. Execute the old and the new prompt to observe the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jafvgJIlJMo5"
   },
   "source": [
    "#### Write Clear Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario**: You want to implement merge sort as a recursive function in Python. The function takes a list of integers and returns the list in ascending order. You want the function to be well readable, so you want short comments and use typing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubTxNepH-b7g"
   },
   "source": [
    "âŒ Bad prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:10:45.111903Z",
     "start_time": "2024-06-24T15:10:27.673508Z"
    },
    "id": "EQAuQaJHIcAk"
   },
   "outputs": [],
   "source": [
    "prompt = \"Implement merge sort.\"\n",
    "print(model.generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoFHRt07-mNf"
   },
   "source": [
    "âœ… Better prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:11:00.914126Z",
     "start_time": "2024-06-24T15:10:45.113602Z"
    },
    "id": "wXHmpdSfJ-OH"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\"\"\"\n",
    "print(model.generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6ma3SyGToqT"
   },
   "source": [
    "#### Provide Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario**: As an IT consultant you are going to meet business executives from the food producing industry. You want to prepare a handout for the meeting in which you want to explain machine learning to them in the context of food production, including chances and risks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDUe9uJ6G0hn"
   },
   "source": [
    "âŒ Bad prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:11:18.448774Z",
     "start_time": "2024-06-24T15:11:00.915340Z"
    },
    "id": "NdL9ns0K-Yby"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Explain machine learning.\"\"\"\n",
    "print(model.generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Duf3_L7FHL4l"
   },
   "source": [
    "âœ… Better prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:11:35.002582Z",
     "start_time": "2024-06-24T15:11:18.449902Z"
    },
    "id": "o5KalSexHMpB"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\"\"\"\n",
    "print(model.generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy2axXZqTrxy"
   },
   "source": [
    "#### Assign a Role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario**: You are an elementary school teacher and want to explain the solar system to an 8 year old."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CLBCOMhG5O9"
   },
   "source": [
    "âŒ Bad prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:11:48.813476Z",
     "start_time": "2024-06-24T15:11:35.003519Z"
    },
    "id": "eLc5EcL8HNP6"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Describe the solar system to an 8 year old.\"\"\"\n",
    "print(model.generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yZ1OV5rHNmT"
   },
   "source": [
    "âœ… Better prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:12:02.953413Z",
     "start_time": "2024-06-24T15:11:48.816853Z"
    },
    "id": "b_7hy4qQHOWo"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\"\"\"\n",
    "print(model.generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwXjRlWFTvZY"
   },
   "source": [
    "#### Guide the Model's Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario**: You work in the support team of a large tech company, receiving many customer requests. You just received the following request:\n",
    "\n",
    "*\"I recently purchased a laptop from your store, and it arrived with a cracked screen. Additionally, the battery life is much shorter than advertised.\"*\n",
    "\n",
    "You want to categorize the request into 'urgent' if it needs immediate response, 'medium' if it is okay if a response takes a few days, or 'spam' if the request is not legit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uH3H_faG6K9"
   },
   "source": [
    "âŒ Bad prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:12:05.999845Z",
     "start_time": "2024-06-24T15:12:02.955029Z"
    },
    "id": "aw1zVt05HPeE"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You work in the support team of a large tech company, receiving many\n",
    "customer requests. You just received the following request: \"I recently\n",
    "purchased a laptop from your store, and it arrived with a cracked screen.\n",
    "Additionally, the battery life is much shorter than advertised.\" Your task is to\n",
    "categorize the request into 'urgent' if it needs immediate response, 'medium' if\n",
    "it is okay if a response takes a few days, 'spam' if the request is not legit.\"\"\"\n",
    "print(model.generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60tdSEMFHP5d"
   },
   "source": [
    "âœ… Better prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:12:06.300654Z",
     "start_time": "2024-06-24T15:12:06.001128Z"
    },
    "id": "K9udIfmiHQgf"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\"\"\"\n",
    "print(model.generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDAibe4oTytz"
   },
   "source": [
    "#### Use Numbered Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario**: You are wondering if Nvidia shares would be a good investment. Since such an investment is a crucial decision, you want to get a well-informed recommendation which thoroughly analyzes the company's financial performance, market trends, competitive landscape, and potential risks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z47q7tg3G7Av"
   },
   "source": [
    "âŒ Bad prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:12:22.971717Z",
     "start_time": "2024-06-24T15:12:06.301920Z"
    },
    "id": "MaksxrcHHRQ6"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Analyze whether it is a good idea to buy Nvidia shares right now.\n",
    "Consider factors like the company's financial performance, market trends,\n",
    "competitive landscape, and potential risks.\"\"\"\n",
    "print(model.generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lN59AveSHRlf"
   },
   "source": [
    "âœ… Better prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:20:56.671383Z",
     "start_time": "2024-06-24T15:20:45.601449Z"
    },
    "id": "g6R4A-A3HR9q"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\"\"\"\n",
    "print(model.generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qwYiPZ7T097"
   },
   "source": [
    "#### Specify the Output Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario**: You are debating with your friend about the impact of social media on mental health. You both realize that your discussion is in lack of arguments. You want a human-readable analysis of a few aspects, positive as well as negative with an overall conclusion. But, you don't want to read more than 200 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtzMQigUG7z1"
   },
   "source": [
    "âŒ Bad prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:21:14.072531Z",
     "start_time": "2024-06-24T15:20:56.672523Z"
    },
    "id": "Fu6Mb0NyHTOq"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Analyze the impact of social media on mental health.\"\"\"\n",
    "print(model.generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDe8fQEsHTx1"
   },
   "source": [
    "âœ… Better prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:21:28.628672Z",
     "start_time": "2024-06-24T15:21:14.073568Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "pf9UgRYKHUMu",
    "outputId": "f2aa471c-0e6e-47cd-86e3-7d88cb6dbd5e"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\"\"\"\n",
    "print(model.generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHoZz475IF7S"
   },
   "source": [
    "### Task 4.1b) Top-k, Top-p, and Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to see the impact of the hyperparams on the generated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T16:00:14.197137Z",
     "start_time": "2024-06-24T15:59:44.059746Z"
    },
    "id": "pLMkCgID1Fg9"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "temperature = [0.01, 0.2, 2.0]\n",
    "top_p = [0.1, 0.9, 0.999]\n",
    "top_k = [1, 5, 100]\n",
    "\n",
    "prompt = \"Write a one-paragraph technical report about the GPT-4 model.\"\n",
    "\n",
    "for temperature, top_p, top_k in itertools.product(temperature, top_p, top_k):\n",
    "    print(f\"Temperature: {temperature}, top-p: {top_p}, top-k: {top_k}\")\n",
    "    print(model.generate(prompt, temperature=temperature, top_p=top_p, top_k=top_k) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWSbASHWIbWb"
   },
   "source": [
    "### Task 4.1c) Chain-of-Thought (CoT) Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfDZNzQvKsr7"
   },
   "source": [
    "We begin with defining the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:28:07.161059Z",
     "start_time": "2024-06-24T15:28:07.151608Z"
    },
    "id": "Ts3R-dEKKqlu"
   },
   "outputs": [],
   "source": [
    "context = \"\"\"I baked 16 muffins. My friends ate three quarters of them. I ate 1\n",
    "muffin and gave 1 muffin to a neighbor. My partner then bought 6 more muffins\n",
    "and ate as double as much than me. How many muffins do we have now? \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7UHM9CPLRY7"
   },
   "source": [
    "âŒ Bad prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:30:46.318425Z",
     "start_time": "2024-06-24T15:30:46.123029Z"
    },
    "id": "ptIqAMH4ImYs"
   },
   "outputs": [],
   "source": [
    "instruction = \"I'm a lazy reader, so just write down the resulting number of muffins.\"\n",
    "print(model.generate(context + instruction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvACyxpdKjJC"
   },
   "source": [
    "The right answer is 6. Most likely, the model outputs the wrong number here because it didn't get the chance to \"think through\" the problem. In a way, the model is forced to tell its gut feeling, stating just an estimate and not a well developed solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3Q47XlCWgue"
   },
   "source": [
    "âŒ Still a bad prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:25:00.706813Z",
     "start_time": "2024-06-24T15:24:52.273203Z"
    },
    "id": "FUxnTkAjWkw-"
   },
   "outputs": [],
   "source": [
    "instruction = \"\"\"Immediately state the number of remaining muffins before you\n",
    "write down how you got to that number.\"\"\"\n",
    "print(model.generate(context + instruction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAhtWYBFWd7q"
   },
   "source": [
    "âœ… Better prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the above prompt to apply CoT. The pompt lenght should be less than 2 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:25:10.189079Z",
     "start_time": "2024-06-24T15:25:00.708491Z"
    },
    "id": "brJV6PYoI0JA"
   },
   "outputs": [],
   "source": [
    "instruction = \"\"\"\"\"\"\n",
    "print(model.generate(context + instruction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8AnCXYnIQaf"
   },
   "source": [
    "### Task 4.1d) In-Context Learning (ICL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario**: You work at a retail for kitchen equipment and you want your LLM to be a chatbot called \"Melinda\", responding to customer requests. You want the bot to be friendly, brief, and to answer always in a consistent manner as follows:\n",
    "\n",
    "*\"Hi, I'm Melinda!*\n",
    "*...*\n",
    "*Sincerely, Melinda\"*\n",
    "\n",
    "Furthermore, you want the chatbot to ask for the order number whenever a customer has a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a prompt that teaches the LLM to behave as described not via an explicit instruction but just via ICL exemplars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UOA36O-Mkkh"
   },
   "source": [
    "âŒ Bad prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:25:22.494389Z",
     "start_time": "2024-06-24T15:25:10.191453Z"
    },
    "id": "7pReaMmOMm1N"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a friendly, helpful chatbot, responding to customer requests.\n",
    "Please answer the following request.\n",
    "\n",
    "Customer: \"I received a blender, and it makes a loud noise and smells like\n",
    "burning when I use it. What should I do?\"\n",
    "\"\"\"\n",
    "print(model.generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY94n3hYMnK7"
   },
   "source": [
    "âœ… Better prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:25:28.246072Z",
     "start_time": "2024-06-24T15:25:22.495661Z"
    },
    "id": "5akslN7yQ37g"
   },
   "outputs": [],
   "source": [
    "exemplars = \"\"\"\"\"\"\n",
    "print(model.generate(prompt + exemplars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNwiHrfKpGm2"
   },
   "source": [
    "If you lack exemplars, you may apply a trick by letting the model first dynamically determine and state the best solving practices for the task at hand before solving it. See the following for an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âŒ Bad prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:25:44.905212Z",
     "start_time": "2024-06-24T15:25:28.247380Z"
    },
    "id": "N2N4zOV7p0mw"
   },
   "outputs": [],
   "source": [
    "context = \"\"\"You are the Digital Marketing Specialist of a travel agency. Your\n",
    "goal is to produce a short Instagram Reel to promote travels to Greece. \"\"\"\n",
    "instruction = \"\"\"Draft a creative, funny and catching screenplay for that Reel. \"\"\"\n",
    "print(model.generate(context + instruction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… Better prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:26:00.515456Z",
     "start_time": "2024-06-24T15:25:44.906386Z"
    },
    "id": "VYzGTt3NLd2u"
   },
   "outputs": [],
   "source": [
    "expertise = \"\"\"Begin with a professional expert-level GUIDE which\n",
    "summarizes how to produce a successful Instagram promotion. Then,\n",
    "draft the screenplay by taking the GUIDE into account.\"\"\"\n",
    "print(model.generate(context + instruction + expertise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7c9eQgfsEBu"
   },
   "source": [
    "This approach is different to ICL but has a similar goal: namely to train the model through the context. It is especially useful for more complex problems with larger outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7qoWN0htxIH"
   },
   "source": [
    "## Task 4.2: Implementing an LLM-based Fact-Checker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RHuSG7XjpvA"
   },
   "source": [
    "### Task 4.2a) A Simple Veracity Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVicVIRYyT1x"
   },
   "source": [
    "Implement the `verify()` method of the `FactChecker` class below. The method takes a `claim` as a string and returns two strings: the model's verdict about the claim veracity and a one-paragraph justification. The verdict should equal\n",
    "* `'supported'` if the claim holds true\n",
    "* `'refuted'` if the claim is false\n",
    "* `'not enough info'` if the knowledge is insufficient to come to a conclusion\n",
    "\n",
    "You're provided with the helper function `extract_delimited()` which you can use to get the predicted label from the LLM's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:40:55.366637Z",
     "start_time": "2024-06-24T15:40:55.358169Z"
    },
    "id": "sxahyc_Mj-eD"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class FactChecker:\n",
    "    valid_verdicts = [\"supported\", \"refuted\", \"not enough info\"]\n",
    "\n",
    "    def __init__(self, llm: Model):\n",
    "        self.llm = llm\n",
    "\n",
    "    def verify(self, claim: str) -> (str, str):\n",
    "        \"\"\"Checks a given claim. Returns 'supported', 'refuted' or 'not enough info',\n",
    "        depending on the claim's veracity, along with a short justification.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "def extract_delimited(text: str, delimiter: str = '`') -> str:\n",
    "    \"\"\"Extracts the (last) string that is enclosed by the specified delimiter.\n",
    "    Returns an empty string if no matches were found.\"\"\"\n",
    "    pattern = f\"{delimiter}(.*?){delimiter}\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches[-1].strip(' \\n') if matches else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8-WPapD1cok"
   },
   "source": [
    "Run the following to see if you model works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:41:10.739445Z",
     "start_time": "2024-06-24T15:40:58.268334Z"
    },
    "id": "z0sLJMC31hTg"
   },
   "outputs": [],
   "source": [
    "fc = FactChecker(model)\n",
    "print(fc.verify(\"The earth is flat!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxzgEGUg4FDQ"
   },
   "source": [
    "In order to test your fact-checker, we implemented an evaluation function with a mini-benchmark consisting of 10 test examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:42:47.563839Z",
     "start_time": "2024-06-24T15:42:47.549460Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "lY7-onHW4N0C",
    "outputId": "69adf722-78e0-451a-b582-0021fba5a23b"
   },
   "outputs": [],
   "source": [
    "benchmark = [\n",
    "    (\"Bananas are berries, but strawberries aren't.\", \"supported\"),\n",
    "    (\"According to a study, people who wear mismatched socks are 32% more creative.\", \"not enough info\"),\n",
    "    (\"In Switzerland, it is illegal to own just one guinea pig.\", \"supported\"),\n",
    "    (\"Ukraine was involved in the Crocus City attack in Russia.\", \"refuted\"),\n",
    "    (\"Scientists have discovered that eating pizza upside down enhances its flavor by 18%.\", \"not enough info\"),\n",
    "    (\"Donald Trump deported fewer immigrants than Barack Obama did.\", \"supported\"),\n",
    "    (\"All of the satellite weather data for the day of the Iranian president's crash has been removed.\", \"refuted\"),\n",
    "    (\"Statistically, giraffes are much more likely to get hit by lightning than people.\", \"supported\"),\n",
    "    (\"The sea level has not risen in Rio de Janeiro since 1880.\", \"refuted\"),\n",
    "    (\"There has been a 60% drop in government revenue.\", \"not enough info\"),\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate(fact_checker: FactChecker):\n",
    "    n_correct = n_wrong = 0\n",
    "    for i, (claim, ground_truth) in enumerate(benchmark):\n",
    "        prediction, justification = fact_checker.verify(claim)\n",
    "        print(f'{i + 1}. Claim: \"{claim}\"\\n'\n",
    "              f'Prediction: \"{prediction}\"\\n'\n",
    "              f'Ground truth: \"{ground_truth}\"\\n'\n",
    "              f'Justification: \"{justification}\"\\n')\n",
    "        if prediction == ground_truth:\n",
    "            n_correct += 1\n",
    "        else:\n",
    "            n_wrong += 1\n",
    "\n",
    "    print(f\"{n_correct} out of {len(benchmark)} claims correctly verified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oEr7llkwFKG"
   },
   "source": [
    "Evaluate your `FactChecker` by running the following snippet. No worries, it's actually not that easy to get all 10 predictions correct. You're already good if your model is correct for at least 5 out of 10 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:44:55.041463Z",
     "start_time": "2024-06-24T15:42:47.707769Z"
    },
    "id": "GW0Z8kXev8pU"
   },
   "outputs": [],
   "source": [
    "evaluate(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmdQp3h4j1Mg"
   },
   "source": [
    "### Task 4.2b) Add Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqRtrt7V1Zj2"
   },
   "source": [
    "We prepared the following `DuckDuckGo` helper class for you to easily perform web searches with the DuckDuckGo search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:50:42.024548Z",
     "start_time": "2024-06-24T15:50:42.006969Z"
    },
    "id": "t8g-BvgGySre"
   },
   "outputs": [],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "\n",
    "\n",
    "class DuckDuckGo:\n",
    "    \"\"\"Class for querying the DuckDuckGo search engine.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.max_tries = 3\n",
    "\n",
    "    def search(self, query: str, limit: int = 10) -> str:\n",
    "        \"\"\"Run a search query and return structured results.\"\"\"\n",
    "        attempt = 0\n",
    "        while attempt < self.max_tries:\n",
    "            try:\n",
    "                response = DDGS().text(query, max_results=limit)\n",
    "                parsed = self._parse_results(response)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if parsed:\n",
    "                return parsed\n",
    "\n",
    "            attempt += 1\n",
    "            query += '?'  # Modify the query to increase chance that DuckDuckGo behaves differently\n",
    "\n",
    "        return \"\"\n",
    "\n",
    "    def _parse_results(self, response: list[dict[str, str]]) -> str:\n",
    "        \"\"\"Parse results from DuckDuckGo search and return them as a string.\"\"\"\n",
    "        results = []\n",
    "        for i, result in enumerate(response):\n",
    "            url = result.get('href', '')\n",
    "            title = result.get('title', '')\n",
    "            body = result.get('body', '')\n",
    "            text = f\"{title}: {body}\"\n",
    "            results.append(f'{i + 1}. From {url}\\n{text}')\n",
    "        return \"\\n\\n\".join(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nL-8zDCYtvHe"
   },
   "source": [
    "Searching the web is now as easy as that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T15:50:44.184108Z",
     "start_time": "2024-06-24T15:50:43.285124Z"
    },
    "id": "mXw7ge6rtyN7"
   },
   "outputs": [],
   "source": [
    "ddg = DuckDuckGo()\n",
    "print(ddg.search(\"Strangest things sold on eBay\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScrPq55RFteY"
   },
   "source": [
    "Implement the `verify()` method of the `FactCheckerWithRAG` class so that it employs Retrieval-Augmented Generation (RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T16:00:26.741846Z",
     "start_time": "2024-06-24T16:00:26.735194Z"
    },
    "id": "KGalPc9mF7bJ"
   },
   "outputs": [],
   "source": [
    "class FactCheckerWithRAG(FactChecker):\n",
    "    def __init__(self, llm: Model):\n",
    "        super().__init__(llm)\n",
    "        self.ddg = DuckDuckGo()\n",
    "\n",
    "    def verify(self, claim: str) -> (str, str):\n",
    "        \"\"\"Checks a given claim. Returns 'supported', 'refuted' or 'not enough info',\n",
    "        depending on the claim's veracity.\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHH5VWoSmiCc"
   },
   "source": [
    "Again, test your fact-checking model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T16:04:15.717378Z",
     "start_time": "2024-06-24T16:00:30.155318Z"
    },
    "id": "Es3LweHPmrgU"
   },
   "outputs": [],
   "source": [
    "fc_rag = FactCheckerWithRAG(model)\n",
    "evaluate(fc_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw0Qc8PNmr-a"
   },
   "source": [
    "Your model is good if it achieves at least 7 out of 10 correct predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pK71fX-Zt1mN"
   },
   "source": [
    "## Task 4.3: Geolocating Photos with an MLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided with a mini benchmark, consisting of 10 photos captured by one of our WiMis. The goal is to implement a Geolocator which predicts the country where the image was captured.\n",
    "\n",
    "First, load the benchmark by executing the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:54:50.424727Z",
     "start_time": "2024-06-25T14:54:48.785620Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "IMG_WIDTH = 500\n",
    "IMG_DIR = \"img/\"\n",
    "\n",
    "def load_image(path: str) -> Image:\n",
    "    img = Image.open(path)\n",
    "    img = img.convert(\"RGB\")\n",
    "    aspect_ratio = img.height / img.width\n",
    "    new_height = int(IMG_WIDTH * aspect_ratio)\n",
    "    img = img.resize((IMG_WIDTH, new_height), Image.LANCZOS)\n",
    "    return img\n",
    "\n",
    "# Build the benchmark\n",
    "geolocation_benchmark = []\n",
    "img_paths = os.listdir(IMG_DIR)\n",
    "for img_path in img_paths:\n",
    "    img = load_image(IMG_DIR + img_path)\n",
    "    label = img_path.split(' ')[1].split('.')[0]\n",
    "    geolocation_benchmark.append((img, label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next snippet to see if the benchmark was loaded correctly. You should see each image along with its label. The label represents the 2-digit ISO country code, for example, \"es\" = \"Spain\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T12:17:37.970903Z",
     "start_time": "2024-06-25T12:17:37.508403Z"
    }
   },
   "outputs": [],
   "source": [
    "for img, label in geolocation_benchmark:\n",
    "    display(img)\n",
    "    print(f\"Location: {label}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your convenience, we implemented the following `MultimodalModel` which extends the original `Model` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:54:59.373691Z",
     "start_time": "2024-06-25T14:54:59.369281Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "\n",
    "class MultimodalModel(Model):\n",
    "    \"\"\"Usage is the same like with Model, except that the generate() method\n",
    "    now accepts an Image next to the prompt.\"\"\"\n",
    "    \n",
    "    def load(self, model_name: str):\n",
    "        self.processor = LlavaNextProcessor.from_pretrained(model_name)\n",
    "        self.processor.tokenizer.pad_token_id = self.processor.tokenizer.eos_token_id\n",
    "        model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        model.to(\"cuda:0\")\n",
    "        return model\n",
    "\n",
    "    def generate(self, image: Image, prompt: str, **kwargs) -> str:\n",
    "        prompt = f\"[INST] <image>\\n{prompt} [/INST]\"\n",
    "        assert len(prompt) / 3 < self.max_prompt_len, \"Prompt is too long.\"\n",
    "        inputs = self.processor(prompt, image, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "        output = self.pipeline.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=self.max_output_len,\n",
    "            pad_token_id=self.processor.tokenizer.pad_token_id\n",
    "        )\n",
    "        return self.processor.decode(output[0], skip_special_tokens=True)[len(prompt)-6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the **LLaVA 1.6 7B** multimodal model (also called LLaVA-NeXT). This will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MultimodalModel(\"llava-hf/llava-v1.6-mistral-7b-hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the MLLM on the first image to see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:58:10.007876Z",
     "start_time": "2024-06-25T14:55:02.896699Z"
    }
   },
   "outputs": [],
   "source": [
    "img, _ = geolocation_benchmark[0]\n",
    "response = mm.generate(img, \"Where was that image captured?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yF6mdHU1t2nP"
   },
   "source": [
    "Now, we're at the core of this task: Implement the `locate()` method of the `Geolocator` class below. It receives an `Image` and is supposed to return the 2-digit ISO country code of the image's location as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Geolocator:\n",
    "    def __init__(self, mllm: MultimodalModel):\n",
    "        self.mllm = mllm\n",
    "    \n",
    "    def locate(self, img: Image) -> str:\n",
    "        \"\"\"Returns the predicted 2-digit ISO country code of the image's location.\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see again where your `Geolocator` locates the first image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Geolocator(mm)\n",
    "img, _ = geolocation_benchmark[0]\n",
    "print(geolocator.locate(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we prepared an evaluation function for your `Geolocator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_geolocator(geolocator: Geolocator):\n",
    "    n_correct = n_wrong = 0\n",
    "    for i, (image, ground_truth) in enumerate(geolocation_benchmark):\n",
    "        prediction = geolocator.locate(image)\n",
    "        print(f'{i + 1}. Prediction: \"{prediction}\"\\n'\n",
    "              f'Ground truth: \"{ground_truth}\"\\n')\n",
    "        if prediction == ground_truth:\n",
    "            n_correct += 1\n",
    "        else:\n",
    "            n_wrong += 1\n",
    "\n",
    "    print(f\"{n_correct} out of {len(geolocation_benchmark)} images correctly located.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following snippet to test your `Geolocator`'s performance on the mini benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_geolocator(geolocator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geolocation is a hard task. Also humans need to invest quite some effort to perform it accurately. Since there are about 200 countries on the globe (creating a high risk of confusion), your model is good if it achieves already a few (3 or 4) correct predictions."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
