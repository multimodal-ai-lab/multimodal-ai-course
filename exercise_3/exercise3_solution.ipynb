{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knoWPzgldj-R"
      },
      "source": [
        "# Exercise 3: **Diffusion Models**\n",
        "\n",
        "In this exercise, you will get some hands-on experience with **Deep Generative Modelling**. There are many different types of deep generative models (VAEs, GANs, Normalizing Flows, etc.), all with different strengths and weaknesses. Over the last few years, the paradigm of Diffusion Models (DMs) has started to dominate the generative modelling landscape, thanks to the impactful work on [DDPMs](https://arxiv.org/pdf/2006.11239) (Ho et al. 2020) that found a super simple parameterisation of DMs revealing an equivalence to another, so far separate, line of research on so-called denoising score-based generative models and showing that DMs can produce diverse samples of very high quality."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ùó Task 3.1: Read the DDPM paper\n",
        "We will focus on DDPMs (Ho et al. 2020) in this exercise. You are supposed to read the [DDPM paper](https://arxiv.org/pdf/2006.11239). The following tasks will ask you to implement the placeholders in this notebook (indicated by üíª), which most of the time have a direct correspondence to equations or algorithms from the paper. After you went through the paper, you can continue with this notebook.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://huggingface.co/blog/assets/78_annotated-diffusion/ddpm_paper.png\" width=\"500\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "VawwrxPt7SJL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg7VMAZ5v46Q"
      },
      "source": [
        "## üöÄ Notebook Requirements\n",
        "First, install and import all the basic packages and dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTn_IxiOTzpJ"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q --upgrade pip\n",
        "!pip3 install -q diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22dGexL2dghl"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import imageio\n",
        "import numpy as np\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision.transforms import Compose, ToTensor, Lambda, Grayscale, Resize\n",
        "from torchvision.datasets.mnist import MNIST, FashionMNIST\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzfySAEvw_2e"
      },
      "source": [
        "## ü™≤ The Datasets\n",
        "For the purpose of this exercise, we added a few different datasets that you can experiment with:\n",
        "- **MNIST**: You all know it! A classical simple grayscale dataset of 28x28 images of handwritten digits (10 classes).\n",
        "- **FashionMNIST**: Similarly to MNIST, this dataset contains 10 classes with 28x28 images, but this time of fashion assets, like shoes, dresses, etc.\n",
        "- **CIFAR100**: Well-known dataset with 100 classes of 32x32 RGB images. The classes cover natural things such as different types of animals, but also objects like planes, cars, trucks, etc.\n",
        "- **CIFAR100Gray**: Same as CIFAR100, just transformed into grayscale. Instead of 3 channels, we only have 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2DCBeOerjMY"
      },
      "source": [
        "### Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVzidytDgPYt"
      },
      "outputs": [],
      "source": [
        "def denormalize_to_zero_to_one(img):\n",
        "    img = img.clamp(-1, 1)\n",
        "    return (img + 1.0) / 2.0\n",
        "\n",
        "\n",
        "def load_dataset(dataset_name, n_classes):\n",
        "\n",
        "  if dataset_name in [\"MNIST\", \"FashionMNIST\"]:\n",
        "      assert n_classes <= 10, \"Please choose n_classes <= 10 for the selected dataset.\"\n",
        "\n",
        "  print(f\"Loading dataset: {dataset_name} (reduced to first {n_classes} classes)\")\n",
        "\n",
        "  if dataset_name == \"CIFAR100\":\n",
        "      img_transform = Compose([\n",
        "          ToTensor(),\n",
        "          Lambda(lambda x: (x - 0.5) * 2)],\n",
        "      )\n",
        "      dataset = torchvision.datasets.CIFAR100(root=\"data\", download=True, transform=img_transform)\n",
        "\n",
        "      def get_class_name(label):\n",
        "          classes = ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
        "          return classes[label]\n",
        "\n",
        "      total_n_classes = 100\n",
        "      image_size = (32, 32)\n",
        "      image_channels = 3\n",
        "\n",
        "  elif dataset_name == \"CIFAR100Gray\":\n",
        "      img_transform = Compose([\n",
        "          Grayscale(),\n",
        "          ToTensor(),\n",
        "          Lambda(lambda x: (x - 0.5) * 2)],\n",
        "      )\n",
        "      dataset = torchvision.datasets.CIFAR100(root=\"data\", download=True, transform=img_transform)\n",
        "\n",
        "      def get_class_name(label):\n",
        "          classes = ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
        "          return classes[label]\n",
        "\n",
        "      total_n_classes = 100\n",
        "      image_size = (32, 32)\n",
        "      image_channels = 1\n",
        "\n",
        "  elif dataset_name == \"FashionMNIST\":\n",
        "      img_transform = Compose([\n",
        "          ToTensor(),\n",
        "          Lambda(lambda x: (x - 0.5) * 2)]\n",
        "      )\n",
        "      dataset = torchvision.datasets.FashionMNIST(\"./datasets\", download=True, train=True, transform=img_transform)\n",
        "\n",
        "      def get_class_name(label):\n",
        "          classes = [\"T-shirt\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "          return classes[label]\n",
        "\n",
        "      total_n_classes = 10\n",
        "      image_size = (28, 28)\n",
        "      image_channels = 1\n",
        "\n",
        "  elif dataset_name == \"MNIST\":\n",
        "      img_transform = Compose([\n",
        "          ToTensor(),\n",
        "          Lambda(lambda x: (x - 0.5) * 2)]\n",
        "      )\n",
        "      dataset = torchvision.datasets.MNIST(\"./datasets\", download=True, train=True, transform=img_transform)\n",
        "\n",
        "      def get_class_name(label):\n",
        "          return label\n",
        "\n",
        "      total_n_classes = 10\n",
        "      image_size = (32, 32)\n",
        "      image_channels = 1\n",
        "\n",
        "  else:\n",
        "      raise NotImplementedError\n",
        "\n",
        "  class_labels = list(range(n_classes))\n",
        "  indices = [idx for idx, target in enumerate(dataset.targets) if target in class_labels]\n",
        "  dataset = torch.utils.data.Subset(dataset, indices)\n",
        "\n",
        "  return dataset, class_labels, get_class_name, image_size, image_channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkJn1XDjS21T"
      },
      "outputs": [],
      "source": [
        "def show_images(images, title=\"\", rows=None):\n",
        "    \"\"\"Shows the provided images as sub-pictures in a grid\"\"\"\n",
        "\n",
        "    # Convert images to CPU numpy arrays if they are PyTorch tensors\n",
        "    if isinstance(images, torch.Tensor):\n",
        "        images = images.detach().cpu()\n",
        "    else:\n",
        "        images = torch.tensor(images)\n",
        "\n",
        "    # Ensure images are in the shape (N, C, H, W)\n",
        "    if images.ndim == 3:\n",
        "        images = images.unsqueeze(1)  # Add channel dimension for grayscale images\n",
        "\n",
        "    # Determine the number of rows and columns\n",
        "    num_images = len(images)\n",
        "    rows = int(num_images ** 0.5) if rows is None else rows\n",
        "    cols = (num_images + rows - 1) // rows  # Ensure all images are included in the grid\n",
        "\n",
        "    # Calculate total cells and the number of padding images needed\n",
        "    total_cells = rows * cols\n",
        "    padding_images = total_cells - num_images\n",
        "\n",
        "    # Create padding images (white images)\n",
        "    if padding_images > 0:\n",
        "        white_image = torch.ones_like(images[0])  # Create a single white image\n",
        "        white_images = white_image.unsqueeze(0).repeat(padding_images, 1, 1, 1)  # Repeat it\n",
        "        images = torch.cat((images, white_images), dim=0)  # Append to the images tensor\n",
        "\n",
        "    # Create a grid of images\n",
        "    grid = torchvision.utils.make_grid(images, nrow=cols, padding=2, pad_value=1)  # pad_value=1 makes padding white\n",
        "\n",
        "    # Convert the grid to a numpy array for displaying\n",
        "    grid_np = grid.permute(1, 2, 0).numpy()\n",
        "\n",
        "    # Display the grid\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(grid_np.clip(0, 1))\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.gcf().set_dpi(150)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwiO8Z2Hqwna"
      },
      "outputs": [],
      "source": [
        "def get_images_per_class(dataset, label, n_images: int = 16):\n",
        "  images = []\n",
        "  for img, img_label in dataset:\n",
        "    if img_label == label:\n",
        "      images.append(img)\n",
        "    if len(images) == n_images:\n",
        "      break\n",
        "  return images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46j5a0LyxVbV"
      },
      "source": [
        "### ‚ùó Task 3.2: Visualize the Datasets\n",
        "\n",
        "Before we get started with our implementation, let's take a look at some images of the different datasets. You can simply choose a dataset from the configuration below and also specify the number of classes that want to use from the dataset. Fewer classes obviously reduce the size of the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDdQuZ2ppYdQ"
      },
      "outputs": [],
      "source": [
        "# @title Dataset Configuration { display-mode: \"form\", run: \"auto\" }\n",
        "# @markdown ### Choose a dataset:\n",
        "dataset_name = \"FashionMNIST\" # @param [\"MNIST\", \"FashionMNIST\", \"CIFAR100\", \"CIFAR100Gray\"] {type:\"string\"}\n",
        "n_classes = 10 # @param {type:\"slider\", min:1, max:100, step:1}\n",
        "\n",
        "dataset, class_labels, get_class_name, image_size, image_channels = load_dataset(dataset_name, n_classes)\n",
        "\n",
        "print(\"Dataset size:\", len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can run the cell below to take a look at the images."
      ],
      "metadata": {
        "id": "vn95a6Jh6mfw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa5sDuv-qEMY"
      },
      "outputs": [],
      "source": [
        "# visualize the chosen dataset\n",
        "for label in class_labels:\n",
        "    class_images = get_images_per_class(dataset, label)\n",
        "    class_row = denormalize_to_zero_to_one(torchvision.utils.make_grid(class_images)).permute(1, 2, 0).numpy()\n",
        "\n",
        "    plt.imshow(class_row)\n",
        "    plt.title(f'{get_class_name(label)}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    print(\" \")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# double-check that you are using \"cuda\" as your device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "id": "xco4-dTrm-rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwjm8V_cbeoE"
      },
      "source": [
        "## ‚ùó Task 3.3: Unconditional Generation\n",
        "As you know from reading the paper, a **DDPM** (Ho et al. 2020) is a Diffusion Model that discretizes the diffusion process into a finite number $T$ of steps and learns to reverse this process by training a conditional deep neural network to estimate the noise that has been added to a sample $x_t$ at time step $t$.\n",
        "\n",
        "### Forward Diffusion Process\n",
        "The **forward diffusion process** $q(x_t\\mid x_{t-1})$, i.e. the process that diffuses the data distribution by adding noise, is fixed and given as a Markov chain whose transition kernel adds Gaussian noise according to a variance schedule $\\beta_1,\\beta_2,...,\\beta_T$ (see Equation 2 in the DDPM paper). The cool thing about the Gaussian definition is that we can directly sample $x_t$ in closed form at any timestep $t$ without having to do it iteratively $x_0 \\rightarrow x_1 \\rightarrow ... \\rightarrow x_t$. This means that given $x_0$, which is the raw sample from the data distribution, and a timestep $t$, we can sample $x_t$ by just sampling from a Gaussian centred around $x_0$ with a variance that depends on $t$. The higher $t$, the larger the variance essentially, but the schedule defines how much the variance changes over time.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://user-images.githubusercontent.com/10695622/174349667-04e9e485-793b-429a-affe-096e8199ad5b.png\" width=\"800\"/>\n",
        "    <br>\n",
        "    <br>\n",
        "    <em> Figure from DDPM paper (https://arxiv.org/abs/2006.11239). </em>\n",
        "<p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C74qdNjT8jKP"
      },
      "source": [
        "#### Variance Schedule\n",
        "The variance schedule is fixed for our DDPM, so we can precompute at once. Here is a simple function that does that for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL1JVoHvlT5B"
      },
      "outputs": [],
      "source": [
        "def get_variance_schedule(min_beta, max_beta, n_steps):\n",
        "  # linear variance schedule\n",
        "  betas = torch.linspace(min_beta, max_beta, n_steps).to(device)\n",
        "  alphas = 1 - betas\n",
        "  alpha_bars = torch.tensor([torch.prod(alphas[:i + 1]) for i in range(len(alphas))])\n",
        "  return betas, alphas, alpha_bars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6HhkqcW87fP"
      },
      "source": [
        "#### üíª Forward Diffusion Process\n",
        "\n",
        "Now it starts to get interesting. Please take a look at equation (4) and Algorithm (1) in the DDPM paper, which show the closed form of the forward process and the way it can be computed.**Text fett markieren**\n",
        "\n",
        "Given $\\overline\\alpha_t$=`alpha_bars[t]`, please complete the function called `sample_xt` below.\n",
        "- `x0` is the original image tensor, `t` is the time step, `alpha_bars` is a precomputed array of numbers, and `epsilon` is an optionally provided noise map\n",
        "- if `epsilon` is not provided, you MUST generate a basic noise map yourself (remember to push it to the same device as `x0`)\n",
        "- this function is supposed to sample $x_t$ from $q(x_t\\mid x_0)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDfsuBjQhNu1"
      },
      "outputs": [],
      "source": [
        "def sample_xt(x0, t, alpha_bars, epsilon=None):\n",
        "    # --------------------- IMPLEMENTATION REQUIRED ------------------------\n",
        "\n",
        "    # image dimensions\n",
        "    n, c, h, w = x0.shape\n",
        "\n",
        "    if epsilon is None:\n",
        "      epsilon = torch.randn(n, c, h, w).to(x0.device)\n",
        "\n",
        "    # variance schedule\n",
        "    alpha_bar_t = alpha_bars[t].reshape(n, 1, 1, 1)\n",
        "\n",
        "    mean = alpha_bar_t.sqrt() * x0\n",
        "    var = (1 - alpha_bar_t).sqrt()\n",
        "\n",
        "    xt = mean + epsilon * var\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    return xt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function visualizes the forward process:"
      ],
      "metadata": {
        "id": "KeYujMv5BQgt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MC0HotFkZKn"
      },
      "outputs": [],
      "source": [
        "def show_forward(sample, alpha_bars, device):\n",
        "    p_list = np.linspace(0, 1, 10)\n",
        "\n",
        "    img, label = sample\n",
        "    x0 = torch.cat([torch.tensor(img).unsqueeze(dim=0)] * len(p_list), dim=0).to(device)\n",
        "\n",
        "    t = torch.tensor([max(0, int(p * n_steps) - 1) for p in p_list]).to(device)\n",
        "    samples = sample_xt(x0, t, alpha_bars, None).cpu()\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(p_list), figsize=(16, 2), sharey=True)\n",
        "    for tt, ax, xt in zip(t, axes, samples):\n",
        "\n",
        "        xt = denormalize_to_zero_to_one(xt).numpy()\n",
        "\n",
        "        ax.imshow(xt.transpose(1, 2, 0).clip(0, 1), cmap='gray' if image_channels == 1 else None)\n",
        "        ax.set_title(f\"$t={tt}$\")\n",
        "        ax.set_axis_off()\n",
        "\n",
        "    plt.suptitle(f\"{get_class_name(label)} class\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can test your function on the dataset that you selected before:"
      ],
      "metadata": {
        "id": "VqS1ULL0BUzs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSV3aJgWk2nE"
      },
      "outputs": [],
      "source": [
        "# some hyperparameters for the visualization\n",
        "min_beta = 10 ** -4\n",
        "max_beta = 0.02\n",
        "n_steps = 100\n",
        "\n",
        "# precompute the variables of the variance schedule\n",
        "_, _, alpha_bars = get_variance_schedule(min_beta, max_beta, n_steps)\n",
        "alpha_bars = alpha_bars.to(device)\n",
        "\n",
        "# show forward process for 3 first samples of the dataset\n",
        "for x in list(iter(dataset))[:3]:\n",
        "  show_forward(x, alpha_bars, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jhUnJ8I4mTO"
      },
      "source": [
        "#### Reverse Diffusion Process\n",
        "The reverse process is the direction parameterized by a neural network $\\epsilon_\\theta$. It is learned based on data!\n",
        "\n",
        "For this $\\epsilon_\\theta$ model, the DDPM authors used a U-Net architecture. We will define the model a bit later, because its structure depends on the dataset you will be choosing."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DDPM Class\n",
        "For convenience, we will create a DDPM wrapper class that unifies the forward and reverse process logic:"
      ],
      "metadata": {
        "id": "BnrEBnCzUihx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg-ATBymND2U"
      },
      "outputs": [],
      "source": [
        "# DDPM class\n",
        "class DDPM(nn.Module):\n",
        "\n",
        "    def __init__(self, model, image_size, image_channels, n_steps=200, min_beta=10 ** -4, max_beta=0.02, device=None):\n",
        "        super(DDPM, self).__init__()\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.image_channels = image_channels\n",
        "        self.n_steps = n_steps\n",
        "\n",
        "        # precompute the variables of the variance schedule\n",
        "        betas, alphas, alpha_bars = get_variance_schedule(min_beta, max_beta, n_steps)\n",
        "        self.betas, self.alphas, self.alpha_bars = betas.to(device), alphas.to(device), alpha_bars.to(device)\n",
        "\n",
        "\n",
        "    def forward(self, x0, t, epsilon=None):\n",
        "        # Forward process (see Section 3.1 in paper)\n",
        "        return sample_xt(x0, t, self.alpha_bars, epsilon)\n",
        "\n",
        "\n",
        "    def reverse(self, x, t):\n",
        "        # Reverse process (see section 3.2 in paper)\n",
        "        return self.model(x, t, return_dict=False)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK_dcpGg9Pus"
      },
      "source": [
        "#### üíª Iterative **Reverse** Sampling\n",
        "One of the original ideas coming from the score-based generative modeling research is to represent a data distribution $p(x)$ by modelling its gradient $\\nabla p(x)$. For sampling, an iterative gradient-based procedure is then applied that follows the local gradients and over time optimizes the likelihood of $x_t$. This is separate from the gradient-based optimization of the parameters $\\theta$ of $\\epsilon_\\theta$, as the sampling procedure is used after the model has been fully trained."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(ddpm: DDPM, n_samples: int):\n",
        "  h, w = ddpm.image_size\n",
        "  c = ddpm.image_channels\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    # --------------------- IMPLEMENTATION REQUIRED ------------------------\n",
        "    x = torch.randn(n_samples, c, h, w).to(device)\n",
        "    # ----------------------------------------------------------------------\n",
        "\n",
        "    for t in list(range(ddpm.n_steps))[::-1]:\n",
        "\n",
        "      # --------------------- IMPLEMENTATION REQUIRED ------------------------\n",
        "      time_tensor = (torch.ones(n_samples) * t).to(device).long()\n",
        "      epsilon_theta = ddpm.reverse(x, time_tensor)\n",
        "\n",
        "      alpha_t = ddpm.alphas[t]\n",
        "      alpha_t_bar = ddpm.alpha_bars[t]\n",
        "\n",
        "      # Partially denoising the image\n",
        "      x = (1 / alpha_t.sqrt()) * (x - (1 - alpha_t) / (1 - alpha_t_bar).sqrt() * epsilon_theta)\n",
        "\n",
        "      # Langevin dynamics\n",
        "      if t > 0:\n",
        "          beta_t = ddpm.betas[t]\n",
        "          sigma_t = beta_t.sqrt()\n",
        "          z = torch.randn(n_samples, c, h, w).to(device)\n",
        "          x = x + sigma_t * z\n",
        "      # ----------------------------------------------------------------------\n",
        "\n",
        "  x = denormalize_to_zero_to_one(x)\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "DiE7w5pXUJb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üíª Training Logic\n",
        "We have all the components that we need at this point except for one. We still need the logic to train our DDPMs. Take another look at Algorithm (1) of the paper and then complete the training loop below by implementing the two placeholders:\n",
        "1. create the loss function\n",
        "2. implement the Algorithm (1) + standard PyTorch training logic"
      ],
      "metadata": {
        "id": "rEzOOQqDERN6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSZ7kifEVRaz"
      },
      "outputs": [],
      "source": [
        "def train(ddpm, loader, n_epochs, optim, device, display=False, store_path=\"ddpm.pt\"):\n",
        "\n",
        "    # --------------------- IMPLEMENTATION REQUIRED ------------------------\n",
        "    mse = nn.MSELoss()\n",
        "    # ----------------------------------------------------------------------\n",
        "\n",
        "    best_loss = float(\"inf\")\n",
        "    n_steps = ddpm.n_steps\n",
        "\n",
        "    batch_losses = []\n",
        "    for epoch in tqdm(range(n_epochs), desc=f\"Training progress\"):\n",
        "        epoch_loss = 0.0\n",
        "        for step, batch in enumerate(tqdm(loader, leave=False, desc=f\"Epoch {epoch + 1}/{n_epochs}\")):\n",
        "\n",
        "            # --------------------- IMPLEMENTATION REQUIRED ------------------------\n",
        "            # Loading data\n",
        "            x0 = batch[0].to(device)\n",
        "            n = len(x0)\n",
        "\n",
        "            # Picking some noise for each of the images in the batch, a timestep and the respective alpha_bars\n",
        "            epsilon = torch.randn_like(x0).to(device)\n",
        "            t = torch.randint(0, n_steps, (n,)).to(device)\n",
        "\n",
        "            # Computing the noisy image based on x0 and the time-step (forward process)\n",
        "            noisy_imgs = ddpm(x0, t, epsilon)\n",
        "\n",
        "            # Getting model estimation of noise based on the images and the time-step\n",
        "            epsilon_theta = ddpm.reverse(noisy_imgs, t)\n",
        "\n",
        "            # Optimizing the MSE between the noise plugged and the predicted noise\n",
        "            loss = mse(epsilon_theta, epsilon)\n",
        "\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            # ----------------------------------------------------------------------\n",
        "\n",
        "            epoch_loss += loss.item() * len(x0) / len(loader.dataset)\n",
        "\n",
        "        # Display images generated at this epoch\n",
        "        if display:\n",
        "            show_images(sample(ddpm, 8), f\"Images generated at epoch {epoch + 1}\")\n",
        "\n",
        "        log_string = f\"Loss at epoch {epoch + 1}: {epoch_loss:.3f}\"\n",
        "\n",
        "        # Storing the model\n",
        "        if best_loss > epoch_loss:\n",
        "            best_loss = epoch_loss\n",
        "            torch.save(ddpm.state_dict(), store_path)\n",
        "            log_string += \" --> Best model ever (stored)\"\n",
        "\n",
        "        print(log_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can test your training loop, choose a dataset as before!"
      ],
      "metadata": {
        "id": "Vp0-_OhvFjCi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPZhSZTPwFd9"
      },
      "outputs": [],
      "source": [
        "# @title Dataset Configuration { display-mode: \"form\", run: \"auto\" }\n",
        "# @markdown ### Choose a dataset:\n",
        "dataset_name = \"FashionMNIST\" # @param [\"MNIST\", \"FashionMNIST\", \"CIFAR100\", \"CIFAR100Gray\"] {type:\"string\"}\n",
        "n_classes = 10 # @param {type:\"slider\", min:1, max:100, step:1}\n",
        "\n",
        "dataset, class_labels, get_class_name, image_size, image_channels = load_dataset(dataset_name, n_classes)\n",
        "\n",
        "print(\"Dataset size:\", len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The U-Net Architecture\n",
        "Here is a nice U-Net implementation from the üß® diffusers library. Feel free to play around with it.\n"
      ],
      "metadata": {
        "id": "YBFJwMYPCBKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DModel\n",
        "\n",
        "# instantiate our U-Net model\n",
        "model = UNet2DModel(\n",
        "  sample_size=image_size, # the target image resolution\n",
        "  in_channels=image_channels, # the number of input channels, 3 for RGB images\n",
        "  out_channels=image_channels, # the number of output channels\n",
        "  layers_per_block=2, # how many ResNet layers to use per UNet block\n",
        "  block_out_channels=(\n",
        "    64,\n",
        "    128,\n",
        "    128,\n",
        "  ), # the number of output channels for each UNet block\n",
        "  down_block_types=(\n",
        "    \"DownBlock2D\", # a regular ResNet downsampling block\n",
        "    \"DownBlock2D\", # a ResNet downsampling block with spatial‚ê£\n",
        "    \"DownBlock2D\", # a regular ResNet downsampling block\n",
        "  ),\n",
        "  up_block_types=(\n",
        "    \"UpBlock2D\", # a regular ResNet upsampling block\n",
        "    \"UpBlock2D\", # a ResNet upsampling block with spatial self-attention\n",
        "    \"UpBlock2D\", # a regular ResNet upsampling block\n",
        "  ))\n",
        "\n",
        "# count the number of parameters to see the model size\n",
        "n_params = sum([p.numel() for p in model.parameters()])\n",
        "print(f\"Created model with {n_params} parameters!\")\n"
      ],
      "metadata": {
        "id": "HRT7G-_0nY6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You now also have choose the hyperparameters for the training:"
      ],
      "metadata": {
        "id": "waW5NsWVwsu6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrCm9DtQuSTy"
      },
      "outputs": [],
      "source": [
        "# @title Training Configuration { display-mode: \"form\", run: \"auto\" }\n",
        "# @markdown ### Enter a name for your training run:\n",
        "run_name = \"ddpm_fashion\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ### Training Hyperparameters:\n",
        "batch_size = 128 # @param {type:\"slider\", min:1, max:512, step:1}\n",
        "n_epochs = 3 # @param {type:\"slider\", min:1, max:50, step:1}\n",
        "lr = 0.7585 # @param {type:\"slider\", min:0.0, max:1.0, step:0.0001}\n",
        "\n",
        "# @markdown ### DDPM Hyperparameters:\n",
        "n_steps = 1001 # @param {type:\"slider\", min:1, max:2000, step:10}\n",
        "min_beta = 0.0001 # @param {type:\"slider\", min:0.0001, max:1.0, step:0.0001}\n",
        "max_beta = 0.02 # @param {type:\"slider\", min:0.0001, max:1.0, step:0.0001}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After you made these choices, you can start the training! Let's see if it works:"
      ],
      "metadata": {
        "id": "jUfynF1EGP5A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNdip8pBWKTs"
      },
      "outputs": [],
      "source": [
        "# instantiate our DDPM, wrapped around our model\n",
        "ddpm = DDPM(model, image_size=image_size, image_channels=image_channels, n_steps=n_steps, min_beta=min_beta, max_beta=max_beta, device=device)\n",
        "\n",
        "# create our dataloader\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# start the training\n",
        "train(ddpm, train_dataloader, n_epochs, optim=torch.optim.AdamW(ddpm.parameters(), lr), display=True, device=device, store_path=f'{run_name}.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "jXpf3wXnGYuG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA5Np1LOMXXt"
      },
      "outputs": [],
      "source": [
        "# specify the model name for inference\n",
        "inference_run_name = \"ddpm_fashion\"\n",
        "\n",
        "ddpm.load_state_dict(torch.load(f'{inference_run_name}.pt', map_location=device))\n",
        "ddpm.eval()\n",
        "print(\"Model loaded:\", inference_run_name)\n",
        "\n",
        "# number of samples to generate\n",
        "N = 64\n",
        "\n",
        "# run the iterative sampling procedure\n",
        "samples = sample(ddpm, N)\n",
        "\n",
        "# show the samples\n",
        "show_images(samples, \"Unconditional Samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OxccXzZwLuZ"
      },
      "source": [
        "# ‚ùó Task 3.4: Conditional Generation\n",
        "The denoising process has so far been totally unconditional, which gave a lot of freedom to the model during training but also zero control to us during inference. Now we want to try to change that and use the classes to condition the reverse process. Our model will then be class-conditional. There are different ways to achieve that, but for this exercise we will just rely on the üß® diffusers library to create learnable class embeddings for us that will be incorporated in the forward pass of the network. Check out the documentation of the [UNet2DModel](https://huggingface.co/docs/diffusers/en/api/models/unet2d) if you want to play around more with.\n",
        "\n",
        "Still, we cannot enforce it to generate images of the given class during inference, but the hope is that the model learns to rely on the additional information, because it helps it to better estimate the noise that was added to a particular sample $x_t$ at time step $t$.\n",
        "\n",
        "First, we will have to upgrade our DDPM class by making sure that the additional class label information $y$ will be propagated to our model in the reverse process."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DDPM class\n",
        "class ClassConditionalDDPM(DDPM):\n",
        "\n",
        "    def reverse(self, x, t, y):\n",
        "        return self.model(x, t, y, return_dict=False)[0]"
      ],
      "metadata": {
        "id": "M4DfYayh3Duc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üíª Conditional Sampling\n",
        "Then we need to extend our sampling function as well. It will now also expect an additional argument called `n_classes`."
      ],
      "metadata": {
        "id": "GX4NFtelWUWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(ddpm: DDPM, n_samples: int, classes: list[int] = None):\n",
        "  h, w = ddpm.image_size\n",
        "  c = ddpm.image_channels\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    # --------------------- IMPLEMENTATION REQUIRED ------------------------\n",
        "    if classes:\n",
        "      n_samples = n_samples * len(classes)\n",
        "\n",
        "    x = torch.randn(n_samples, c, h, w).to(device)\n",
        "    # ----------------------------------------------------------------------\n",
        "\n",
        "    for t in list(range(ddpm.n_steps))[::-1]:\n",
        "\n",
        "      # --------------------- IMPLEMENTATION REQUIRED ------------------------\n",
        "      time_tensor = (torch.ones(n_samples) * t).to(device).long()\n",
        "\n",
        "      if classes:\n",
        "        y = torch.cat([torch.ones(n_samples // len(classes)) * label for label in classes]).long().to(device)\n",
        "        epsilon_theta = ddpm.reverse(x, time_tensor, y)\n",
        "      else:\n",
        "        epsilon_theta = ddpm.reverse(x, time_tensor)\n",
        "\n",
        "      alpha_t = ddpm.alphas[t]\n",
        "      alpha_t_bar = ddpm.alpha_bars[t]\n",
        "\n",
        "      # Partially denoising the image\n",
        "      x = (1 / alpha_t.sqrt()) * (x - (1 - alpha_t) / (1 - alpha_t_bar).sqrt() * epsilon_theta)\n",
        "\n",
        "      # Langevin dynamics\n",
        "      if t > 0:\n",
        "          beta_t = ddpm.betas[t]\n",
        "          sigma_t = beta_t.sqrt()\n",
        "          z = torch.randn(n_samples, c, h, w).to(device)\n",
        "          x = x + sigma_t * z\n",
        "      # ----------------------------------------------------------------------\n",
        "\n",
        "  x = samples = denormalize_to_zero_to_one(x)\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "gKtAvu6lOdED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üíª Conditional DDPM Training\n",
        "Same applies to the training loop. The overall logic is the exact same as before. You just have to get the class information from the batch (hint: `y=batch[1]`) and make sure it is actually used."
      ],
      "metadata": {
        "id": "JwpjfuHjWuiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classconditional(ddpm, loader, n_epochs, optim, device, display=False, store_path=\"ddpm_model.pt\"):\n",
        "\n",
        "    # --------------------- IMPLEMENTATION REQUIRED ------------------------\n",
        "    mse = nn.MSELoss()\n",
        "    # ----------------------------------------------------------------------\n",
        "\n",
        "    best_loss = float(\"inf\")\n",
        "    n_steps = ddpm.n_steps\n",
        "\n",
        "    batch_losses = []\n",
        "    for epoch in tqdm(range(n_epochs), desc=f\"Training progress\"):\n",
        "        epoch_loss = 0.0\n",
        "        for step, batch in enumerate(tqdm(loader, leave=False, desc=f\"Epoch {epoch + 1}/{n_epochs}\")):\n",
        "\n",
        "            # --------------------- IMPLEMENTATION REQUIRED ------------------------\n",
        "            # Loading data\n",
        "            x0 = batch[0].to(device)\n",
        "            y = batch[1].to(device)\n",
        "            n = len(x0)\n",
        "\n",
        "            # Picking some noise for each of the images in the batch, a timestep and the respective alpha_bars\n",
        "            epsilon = torch.randn_like(x0).to(device)\n",
        "            t = torch.randint(0, n_steps, (n,)).to(device)\n",
        "\n",
        "            # Computing the noisy image based on x0 and the time-step (forward process)\n",
        "            noisy_imgs = ddpm(x0, t, epsilon)\n",
        "\n",
        "            # Getting model estimation of noise based on the images and the time-step\n",
        "            epsilon_theta = ddpm.reverse(noisy_imgs, t, y)\n",
        "\n",
        "            # Optimizing the MSE between the noise plugged and the predicted noise\n",
        "            loss = mse(epsilon_theta, epsilon)\n",
        "\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            # ----------------------------------------------------------------------\n",
        "\n",
        "            epoch_loss += loss.item() * len(x0) / len(loader.dataset)\n",
        "\n",
        "        # Display images generated at this epoch\n",
        "        if display:\n",
        "            is_conditional = ddpm.model.class_embedding is not None\n",
        "            show_images(sample(ddpm, 8, n_classes if is_conditional else None), f\"Images generated at epoch {epoch + 1}\")\n",
        "\n",
        "        log_string = f\"Loss at epoch {epoch + 1}: {epoch_loss:.3f}\"\n",
        "\n",
        "        # Storing the model\n",
        "        if best_loss > epoch_loss:\n",
        "            best_loss = epoch_loss\n",
        "            torch.save(ddpm.state_dict(), store_path)\n",
        "            log_string += \" --> Best model ever (stored)\"\n",
        "\n",
        "        print(log_string)"
      ],
      "metadata": {
        "id": "-I5kY10o1IGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DModel\n",
        "\n",
        "# instantiate our U-Net model\n",
        "cond_model = UNet2DModel(\n",
        "\n",
        "  num_class_embeds=n_classes, # ONLY THIS HERE CHANGED!\n",
        "\n",
        "  sample_size=image_size, # the target image resolution\n",
        "  in_channels=image_channels, # the number of input channels, 3 for RGB images\n",
        "  out_channels=image_channels, # the number of output channels\n",
        "  layers_per_block=2, # how many ResNet layers to use per UNet block\n",
        "  block_out_channels=(\n",
        "    64,\n",
        "    128,\n",
        "    128,\n",
        "  ), # the number of output channels for each UNet block\n",
        "  down_block_types=(\n",
        "    \"DownBlock2D\", # a regular ResNet downsampling block\n",
        "    \"DownBlock2D\", # a ResNet downsampling block with spatial‚ê£\n",
        "    \"DownBlock2D\", # a regular ResNet downsampling block\n",
        "  ),\n",
        "  up_block_types=(\n",
        "    \"UpBlock2D\", # a regular ResNet upsampling block\n",
        "    \"UpBlock2D\", # a ResNet upsampling block with spatial self-attention\n",
        "    \"UpBlock2D\", # a regular ResNet upsampling block\n",
        "  ))\n",
        "\n",
        "# count the number of parameters to see the model size\n",
        "n_params = sum([p.numel() for p in model.parameters()])\n",
        "print(f\"Created model with {n_params} parameters!\")\n"
      ],
      "metadata": {
        "id": "7SpOgbHo3DA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Conditional Training Configuration { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "# @markdown ### Enter a name for your training run:\n",
        "cond_run_name = \"conditional_ddpm_fashion\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ### Training Hyperparameters:\n",
        "batch_size = 128 # @param {type:\"slider\", min:1, max:512, step:1}\n",
        "n_epochs = 1 # @param {type:\"slider\", min:1, max:50, step:1}\n",
        "lr = 0.001 # @param {type:\"slider\", min:0.0, max:1.0, step:0.0001}\n",
        "\n",
        "# @markdown ### DDPM Hyperparameters:\n",
        "n_steps = 500 # @param {type:\"slider\", min:1, max:2000, step:10}\n",
        "min_beta = 0.0001 # @param {type:\"slider\", min:0.0001, max:1.0, step:0.0001}\n",
        "max_beta = 0.02 # @param {type:\"slider\", min:0.0001, max:1.0, step:0.0001}"
      ],
      "metadata": {
        "id": "Au-e8I1Pd2CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate our DDPM, wrapped around our model\n",
        "cond_ddpm = ClassConditionalDDPM(cond_model, image_size=image_size, image_channels=image_channels, n_steps=n_steps, min_beta=min_beta, max_beta=max_beta, device=device)\n",
        "\n",
        "# create our dataloader\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# start the training\n",
        "train_classconditional(cond_ddpm, train_dataloader, n_epochs, optim=torch.optim.AdamW(cond_ddpm.parameters(), lr), display=False, device=device, store_path=f'{cond_run_name}.pt')"
      ],
      "metadata": {
        "id": "nPmsy_YP07TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conditional Inference"
      ],
      "metadata": {
        "id": "K71hUNT_ax3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the trained model\n",
        "cond_ddpm.load_state_dict(torch.load(f'{cond_run_name}.pt', map_location=device))\n",
        "cond_ddpm.eval()\n",
        "print(\"Model loaded:\", cond_run_name)"
      ],
      "metadata": {
        "id": "xM5yJxPCfOjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = sample(cond_ddpm, 4, class_labels)\n",
        "show_images(samples, \"Conditional Samples\", rows=len(class_labels))"
      ],
      "metadata": {
        "id": "a9Z3ty-gpyND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in case of too many classes use this approach to avoid memory issues\n",
        "for label in range(n_classes):\n",
        "  print(get_class_name(label))\n",
        "  samples = sample(cond_ddpm, 4, [label])\n",
        "  show_images(samples, \"Conditional Samples\", rows=1)"
      ],
      "metadata": {
        "id": "wcCOX3mIfPX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ùó Task 3.5: Explore!\n",
        "Go back in the notebook and play around with different hyperparameters and modelling choices. Can you make it work with the more challenging CIFAR100 dataset? You can also try to change the architecture of the $\\epsilon_\\theta$ model, i.e. change the parameters of the [UNet2D](https://huggingface.co/docs/diffusers/en/api/models/unet2d) class. Feel also free to share some of your favorite samples in the forum on Moodle."
      ],
      "metadata": {
        "id": "kbOuPcHpttSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further Readings\n",
        "\n",
        "For more insights on the connections to score-based generative modelling, you can take a look at these great posts:\n",
        "\n",
        "- [Score-based Perspective](https://yang-song.net/blog/2021/score/).\n",
        "- [Diffusion Model Perspective](https://calvinyluo.com/2022/08/26/diffusion-tutorial.html)"
      ],
      "metadata": {
        "id": "CuKNA_aA3QXd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8YcbsCdEw5ko"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}